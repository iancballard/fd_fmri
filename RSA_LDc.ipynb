{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from moss.mosaic import Mosaic\n",
    "import nibabel as nib\n",
    "import multiprocessing\n",
    "#os and i/o\n",
    "import os\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preliminary housekeeping\n",
    "home_dir = '/data/home/iballard/fd/'\n",
    "subj_file = home_dir + 'subjects.txt'\n",
    "subs = list(np.loadtxt(subj_file,'string'))\n",
    "subs = ['fd_104']\n",
    "os.chdir(home_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_invertible(a):\n",
    "    return a.shape[0] == a.shape[1] and np.linalg.matrix_rank(a) == a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_inv_shrunk_covariance(x):\n",
    "    #see http://www.diedrichsenlab.org/pubs/Walther_Neuroimage_2016.pdf\n",
    "    t,n = x.shape #t measurements by n voxels\n",
    "\n",
    "    #demean\n",
    "    x = x - x.mean(0)\n",
    "\n",
    "    #compute covariance\n",
    "    sample = (1.0/t) * np.dot(np.transpose(x),x)\n",
    "\n",
    "    #copute prior\n",
    "    prior = np.diag(np.diag(sample))\n",
    "\n",
    "    #compute shrinkage\n",
    "    d = 1.0/n * np.linalg.norm(sample - prior,ord = 'fro')**2\n",
    "    y = np.square(x)\n",
    "    r2 = 1.0/n/t**2 * np.sum(np.sum(np.dot(np.transpose(y),y)))- \\\n",
    "    1.0/n/t*np.sum(np.sum(np.square(sample)))\n",
    "\n",
    "    #compute the estimator\n",
    "    shrinkage = max(0,min(1,r2/d))\n",
    "    sigma = shrinkage*prior + (1-shrinkage)*sample\n",
    "\n",
    "    #compute the inverse\n",
    "    if is_invertible(sigma):\n",
    "        inv_sigma = np.linalg.inv(sigma)\n",
    "    else:\n",
    "        inv_sigma = np.linalg.inv(prior) #univariate\n",
    "    \n",
    "    return inv_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make sure MTL masks are exclusive\n",
    "def trim_mask(mask,m):\n",
    "    exclusions = overlap_masks[:]\n",
    "    exclusions.remove(m)\n",
    "\n",
    "    m1 = op.abspath('./data/' + sub + '/masks/' + exclusions[0] +'.nii.gz')\n",
    "    m1 = nib.load(m1).get_data().astype(bool)\n",
    "\n",
    "    m2 = op.abspath('./data/' + sub + '/masks/' + exclusions[1] +'.nii.gz')\n",
    "    m2 = nib.load(m2).get_data().astype(bool)\n",
    "\n",
    "    bad = np.logical_or(m1,m1)\n",
    "    good = np.invert(bad)\n",
    "\n",
    "    mask = np.logical_and(mask,good)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_inverse_sigma(sub,exp,smooth,masks):\n",
    "    sub_path = op.join(home_dir,'analysis', exp, sub, 'reg','epi', smooth)\n",
    "    for run in map(str,range(1,4)):\n",
    "        res = op.join(sub_path, 'run_'  + run,'res4d_xfm.nii.gz')\n",
    "        res = nib.load(res).get_data().astype(float)\n",
    "\n",
    "        for m in masks:\n",
    "            mask = op.join(home_dir,'data', sub,  'masks', m + '.nii.gz')\n",
    "            mask = nib.load(mask).get_data().astype(bool)\n",
    "            if m in overlap_masks:\n",
    "                mask = trim_mask(mask,m)\n",
    "\n",
    "            x = res[mask]\n",
    "            x = np.transpose(x)\n",
    "\n",
    "            inv_sigma = compute_inv_shrunk_covariance(x)\n",
    "\n",
    "            inv_sigma = scipy.linalg.fractional_matrix_power(inv_sigma,.5) #take square root\n",
    "\n",
    "            out_f = op.join(home_dir,'covariance','_'.join([exp,sub,run,m]) + '.txt')\n",
    "            np.savetxt(out_f,inv_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_inverse_sigma(sub,exp,smooth,masks):\n",
    "    for run in map(str,range(1,4)):\n",
    "        for m in masks:\n",
    "            out_f = op.join(home_dir,'covariance','_'.join([exp,sub,run,m]) + '.txt')\n",
    "            os.remove(out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_betas(sub,exp,smooth,masks):\n",
    "    betas = {'sub':[],'mask':[],'run':[],'condition':[],'value':[],'voxel':[],'row':[]}\n",
    "    for m in masks:\n",
    "        out_f = op.join(home_dir,'betas', '_'.join([exp,smooth,m]) + '.csv')\n",
    "\n",
    "        for sub in subs:\n",
    "            sub_path = op.join(home_dir,'analysis', exp, sub, 'reg','epi', smooth )\n",
    "\n",
    "            mask = op.join(home_dir,'data', sub,  'masks', m + '.nii.gz')\n",
    "            mask = nib.load(mask).get_data().astype(bool)\n",
    "            if m in overlap_masks:\n",
    "                mask = trim_mask(mask,m)\n",
    "\n",
    "            for run in map(str,range(1,4)):\n",
    "                run_dir = op.join(sub_path, 'run_'  + run)\n",
    "\n",
    "                if os.path.exists(run_dir):\n",
    "\n",
    "                    for i in range(1,5):\n",
    "                        f = run_dir + '/cope' + str(i) + '_xfm.nii.gz'\n",
    "                        cond = cond_map[i]\n",
    "\n",
    "                        #load stat image\n",
    "                        stat = nib.load(f).get_data().astype(float)\n",
    "                        stat = stat[mask]\n",
    "\n",
    "                        for n,val in enumerate(stat):\n",
    "                            betas['voxel'].append(n)                        \n",
    "                            betas['sub'].append(sub)\n",
    "                            betas['value'].append(val)\n",
    "                            betas['mask'].append(m)\n",
    "                            betas['run'].append(run)\n",
    "                            betas['condition'].append(cond)\n",
    "                            betas['row'].append(i)\n",
    "                else:\n",
    "                    print run_dir\n",
    "\n",
    "    betas = pd.DataFrame(betas)\n",
    "    betas = betas.set_index(['sub', 'run','mask','condition'])\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prewhiten_betas(sub,betas,masks):\n",
    "    for run in map(str,range(1,4)):\n",
    "        for m in masks:\n",
    "            #load covariance and take root\n",
    "            inv_sigma = op.join(home_dir,'covariance','_'.join([exp,sub,run,m]) + '.txt')\n",
    "            inv_sigma = np.loadtxt(inv_sigma)\n",
    "            \n",
    "            for cond in cond_map.values():\n",
    "                vals = betas.loc[(sub,run,m,cond),'value'].values\n",
    "                whiten_vals = np.dot(vals,inv_sigma)\n",
    "                betas.loc[(sub,run,m,cond),'value'] = whiten_vals\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rsa(sub,exp,masks):\n",
    "    rsas = {}\n",
    "    for m in masks:\n",
    "        \n",
    "        runs = list(set(map(lambda x: x[0],betas.loc[(sub)].index)))\n",
    "        nruns = len(runs)\n",
    "        rsa = np.zeros((nconds,nconds))\n",
    "\n",
    "        for i in range(1,nconds+1):\n",
    "            for j in range(1,nconds+1):\n",
    "                corr = []\n",
    "                \n",
    "                corr.append(scipy.stats.pearsonr(betas.loc[(sub,'1',m,cond_map[i]),'value'].values,\n",
    "                                     betas.loc[(sub,'2',m,cond_map[j]),'value'].values))\n",
    "                corr.append(scipy.stats.pearsonr(betas.loc[(sub,'2',m,cond_map[i]),'value'].values,\n",
    "                                     betas.loc[(sub,'1',m,cond_map[j]),'value'].values))\n",
    "\n",
    "                if nruns == 3:\n",
    "\n",
    "                    corr.append(scipy.stats.pearsonr(betas.loc[(sub,'1',m,cond_map[i]),'value'].values,\n",
    "                                         betas.loc[(sub,'3',m,cond_map[j]),'value'].values))\n",
    "                    corr.append(scipy.stats.pearsonr(betas.loc[(sub,'3',m,cond_map[i]),'value'].values,\n",
    "                                         betas.loc[(sub,'1',m,cond_map[j]),'value'].values))\n",
    "                    corr.append(scipy.stats.pearsonr(betas.loc[(sub,'2',m,cond_map[i]),'value'].values,\n",
    "                                         betas.loc[(sub,'3',m,cond_map[j]),'value'].values))\n",
    "                    corr.append(scipy.stats.pearsonr(betas.loc[(sub,'3',m,cond_map[i]),'value'].values,\n",
    "                                         betas.loc[(sub,'2',m,cond_map[j]),'value'].values))\n",
    "\n",
    "            \n",
    "                       \n",
    "                rsa[i-1,j-1]  = np.nanmean(corr)\n",
    "        rsas[m] = rsa\n",
    "        \n",
    "    return rsas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-121-077d14443056>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-121-077d14443056>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    for sub in subs{}\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# masks = ['peri_sim','para_sim','hipp','hipp_right','hipp_left',\n",
    "#          'entorhinal']\n",
    "masks = ['hipp']\n",
    "overlap_masks = ['peri_sim','para_sim','hipp']\n",
    "exp = 'sim_4mm-onebeta'\n",
    "smooth = 'smoothed'\n",
    "\n",
    "cond_map = {1:'AB+',2:'AC-',3:'B-',4:'C+'}\n",
    "nconds = len(cond_map.keys())\n",
    "\n",
    "subs = ['fd_104']\n",
    "\n",
    "for sub in subs{}\n",
    "    compute_inverse_sigma(sub,exp,smooth,masks)\n",
    "    betas = extract_betas(sub,exp,smooth,masks)\n",
    "\n",
    "    betas = prewhiten_betas(sub,betas,masks)\n",
    "    # delete_inverse_sigma(sub,exp,smooth,masks) #save disk space\n",
    "    rsas = compute_rsa(sub,exp,masks)\n",
    "\n",
    "    for m in masks:\n",
    "        all_rsas[m].append(rsas[m])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entorhinal': [],\n",
       " 'hipp': [],\n",
       " 'hipp_left': [],\n",
       " 'hipp_right': [],\n",
       " 'para_sim': [],\n",
       " 'peri_sim': []}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rsas = {k: [] for k in masks}\n",
    "all_rsas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for sub in subs:\n",
    "#     for m in ['hipp']:\n",
    "        \n",
    "#         #Get crossval folds. Deal with unequal number of runs across subjects\n",
    "#         runs = list(set(map(lambda x: x[0],betas.loc[(sub)].index)))\n",
    "#         nruns = len(runs)\n",
    "#         if nruns == 3:\n",
    "#             train_runs = [['1','2'],['1','3'],['2','3']]\n",
    "#         elif nruns == 2:\n",
    "#             train_runs = [['1'],['2']]\n",
    "        \n",
    "#         rsa = np.zeros((4,4,len(runs)))\n",
    "\n",
    "#         #loop through condition pairs\n",
    "#         for i in range(1,rsa.shape[0]+1):\n",
    "#             for j in range(1,rsa.shape[0]+1):\n",
    "#                 if j < i:\n",
    "#                     print i,j\n",
    "#                     #get euclidian distance between conditions for each run\n",
    "#                     run_euclid_dist = {}\n",
    "#                     for run in map(str,range(1,nruns+1)):\n",
    "                        \n",
    "#                         cond1 = betas.loc[(sub,run,m,cond_map[i]),'value'].values\n",
    "#                         cond2 = betas.loc[(sub,run,m,cond_map[j]),'value'].values\n",
    "                        \n",
    "#                         run_euclid_dist[run] = cond1 - cond2\n",
    "                    \n",
    "#                     #do cross-validation\n",
    "#                     for n,train_run in enumerate(train_runs):\n",
    "#                         train_vector = []\n",
    "#                         test_vector = []\n",
    "                    \n",
    "#                         for run in map(str,range(1,nruns+1)): \n",
    "#                             if run in train_run:\n",
    "#                                 train_vector.append(run_euclid_dist[run])\n",
    "#                             else:\n",
    "#                                 test_vector.append(run_euclid_dist[run])\n",
    "\n",
    "#                         train_vector = np.mean(train_vector,0)\n",
    "#                         test_vector = np.mean(test_vector,0)\n",
    "                        \n",
    "#                         diff = np.dot(train_vector,test_vector)\n",
    "                        \n",
    "#                         rsa[i-1,j-1,n] = diff\n",
    "# # rsa = rsa.mean(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03905492,  0.13804446,  0.11054621,  0.11285011],\n",
       "       [ 0.13804446,  0.17733881,  0.16358399,  0.16732785],\n",
       "       [ 0.11054621,  0.16358399,  0.23519571,  0.22827077],\n",
       "       [ 0.11285011,  0.16732785,  0.22827077,  0.26099641]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
