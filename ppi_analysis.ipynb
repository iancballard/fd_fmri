{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Sets up design files for PPI analysis for use in lyman"
=======
    "#Sets up design files for PPI analysis for use in lyman"
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
<<<<<<< HEAD
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iballard/anaconda/lib/python2.7/site-packages/IPython/parallel.py:13: ShimWarning: The `IPython.parallel` package has been deprecated. You should import from ipyparallel instead.\n",
      "  \"You should import from ipyparallel instead.\", ShimWarning)\n"
     ]
    }
   ],
=======
    "collapsed": true
   },
   "outputs": [],
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   "source": [
    "#os and i/o\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from os.path import abspath\n",
    "import csv\n",
    "\n",
    "#scientific computing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats, optimize\n",
    "from pandas import DataFrame, Series\n",
    "from moss import glm\n",
    "import seaborn as sns\n",
    "import random as rd\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import scipy.stats\n",
    "\n",
    "#ipython add-ons\n",
    "from IPython.parallel import Client\n",
    "from IPython.display import Image\n",
    "import multiprocessing\n",
    "\n",
    "##nipype\n",
    "import nibabel as nib\n",
    "from nipype.pipeline.engine import Node, MapNode, Workflow\n",
    "from nipype.interfaces.io import DataGrabber, DataFinder, DataSink\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.interfaces.fsl import ImageMeants\n",
    "from nipype.interfaces.fsl import ImageStats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preliminary housekeeping\n",
    "home_dir = '/data/home/iballard/fd/'\n",
    "subj_file = home_dir + 'subjects.txt'\n",
    "sub_list = list(np.loadtxt(subj_file,'string'))\n",
    "os.chdir(home_dir)\n",
    "exps = ['sim','ser']\n",
    "runs = map(str,range(1,4))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 61,
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_rejection(a,b):\n",
    "    return a - (np.dot(a,b)/np.dot(b,b) * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Extract timeseries from the mask"
=======
    "#Extract timeseries from the mask"
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/iballard/fd/data/fd_104/masks/extractions/fd_104sim1hipp.txt\n"
     ]
    }
   ],
=======
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   "source": [
    "def extract_roi(in_tuple):\n",
    "    sub,exp,run,mask = in_tuple\n",
    "    \n",
    "    sub_path = home_dir + 'analysis/' + exp + '_4mm/' + sub + '/preproc/run_' + \\\n",
    "    run + '/'\n",
    "\n",
    "    #make sure to get coregistered preproc data\n",
    "    preproc_data = home_dir + 'analysis/' + exp + '_4mm/' + sub + '/reg/epi/unsmoothed/run_' \\\n",
    "    + str(run) + '/timeseries_xfm.nii.gz'\n",
    "\n",
<<<<<<< HEAD
    "    mask_dir = home_dir + 'data/' + sub + '/masks/' \n",
=======
    "    mask_dir = home_dir + 'data/' + sub + '/masks/vta/' \n",
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
    "    out_dir = mask_dir + 'extractions/'\n",
    "    \n",
    "    if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "\n",
<<<<<<< HEAD
    "    mask_file = mask_dir + mask + '.nii.gz'\n",
    "    out_f = out_dir + ('').join(map(str,in_tuple))+ '.txt'\n",
    "    print out_f\n",
=======
    "    mask_file = mask_dir + mask + '_mask.nii.gz'\n",
    "    out_f = out_dir + ('').join(map(str,in_tuple))+ '.txt'\n",
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
    "\n",
    "    if os.path.exists(sub_path):# and not os.path.exists(out_f):\n",
    "        meants = ImageMeants(in_file = preproc_data, eig = True, order = 1, \n",
    "                             mask = mask_file, out_file = out_f)\n",
<<<<<<< HEAD
    "        meants.run()\n",
    "\n",
    "extract_roi(('fd_104','sim','1','hipp'))"
=======
    "        meants.run()"
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 105,
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_roi_prob(in_tuple):\n",
    "    sub,exp,run,mask = in_tuple\n",
    "    \n",
    "    sub_path = home_dir + 'analysis/' + exp + '_4mm/' + sub + '/preproc/run_' + \\\n",
    "    run + '/'\n",
    "\n",
    "    #make sure to get coregistered preproc data\n",
    "    preproc_data = home_dir + 'analysis/' + exp + '_4mm/' + sub + '/reg/epi/unsmoothed/run_' \\\n",
    "    + str(run) + '/timeseries_xfm.nii.gz'\n",
    "\n",
    "    mask_dir = home_dir + 'data/' + sub + '/masks/' + mask + '/' \n",
    "    out_dir = mask_dir + 'extractions/'\n",
    "\n",
    "    prob_file = mask_dir + exp + '_' + mask + '_func_space.nii.gz'\n",
    "    mask_file = mask_dir + exp + '_' + mask + '_mask.nii.gz'\n",
    "    out_f = out_dir + ('').join(map(str,in_tuple))+ '.txt'\n",
    "    tmp_out = mask_dir + sub + exp + run + '.nii.gz'\n",
    "\n",
    "    if os.path.exists(sub_path):# and not os.path.exists(out_f):\n",
    "        cmd = ['fslmaths',preproc_data,'-mul',prob_file,tmp_out]\n",
    "        cmd = ' '.join(cmd)\n",
    "        os.system(cmd)\n",
    "        \n",
    "        meants = ImageMeants(in_file = tmp_out, eig = True, order = 1, \n",
    "                             mask = mask_file, out_file = out_f)\n",
    "        meants.run()\n",
    "        os.remove(tmp_out)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 106,
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "exps = ['sim']\n",
    "rois = ['hipp']\n",
    "in_tuples = []\n",
    "for sub in sub_list:\n",
=======
    "exps = ['ser','sim']\n",
    "rois = ['vta']\n",
    "in_tuples = []\n",
    "for sub in ['fd_104']:#sub_list:\n",
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
    "    for exp in exps:\n",
    "        for run in runs:\n",
    "            for mask in rois:\n",
    "                in_tuples.append((sub,exp,run,mask))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 107,
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(processes = 14)\n",
<<<<<<< HEAD
    "pool.map(extract_roi,in_tuples)\n",
=======
    "pool.map(extract_roi_prob,in_tuples)\n",
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
    "pool.terminate()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Set up design matrix"
=======
    "#Set up design matrix"
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 121,
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# def write_design(in_tuple):\n",
    "#     mask = 'hipp'\n",
    "#     sub,exp = in_tuple\n",
    "#     #hrf params\n",
    "#     hrf = getattr(glm,'GammaDifferenceHRF')\n",
    "#     tr = 1.5\n",
    "#     hrf = hrf(tr = tr)\n",
    "\n",
    "#     out_f = home_dir + 'data/' + sub + '/design/ppi_regressors_' + exp + '_' + mask + '.csv' #out file\n",
    "\n",
    "#     if True:#not os.path.exists(out_f):\n",
    "#         #load design data for this subjects \n",
    "#         design_dir = home_dir + 'data/' + sub + '/design/'\n",
    "#         design_file = design_dir + exp + '_ppi.csv'\n",
    "#         design_data = pd.read_csv(design_file)\n",
    "\n",
    "#         #load in pre-existing noise regressors\n",
    "#         reg_file = design_dir + 'noise_regressors_' + exp + '.csv'\n",
    "#         regressors = pd.read_csv(reg_file)\n",
    "\n",
    "#         #initialize vars to fill\n",
    "#         convolved_ev = []\n",
    "#         ts = []\n",
    "#         for run in runs:\n",
    "#             sub_file = home_dir + 'analysis/' + exp + '_4mm/' + sub + '/preproc/run_' + str(run) + '/unsmoothed_timeseries.nii.gz'\n",
    "\n",
    "#             if os.path.exists(sub_file):\n",
    "#                 ntp = nib.load(sub_file).shape[-1] #get number of time points\n",
    "#                 design = design_data[design_data['run']==int(run)]\n",
    "\n",
    "#                 model = glm.DesignMatrix(design = design, tr = tr, ntp = ntp, hrf_model = hrf, hpf_cutoff = 128)\n",
    "#                 convolved_ev.extend(model.design_matrix['state'].values) #get timeseries for regressor of interest\n",
    "\n",
    "#                 #load ts data\n",
    "#                 mask_dir = home_dir + 'data/' + sub + '/masks/vta/extractions/' \n",
    "#                 fid = (sub,exp,run,mask)\n",
    "#                 mask_f = mask_dir + ('').join(map(str,fid))+ '.txt'\n",
    "#                 roi_ts = np.loadtxt(mask_f)\n",
    "#                 roi_ts = roi_ts - np.mean(roi_ts) #mean center\n",
    "#                 ts.extend(roi_ts)\n",
    "\n",
    "#         #update regressors dataframe\n",
    "#         ts = scipy.stats.zscore(ts) #add ts to the regressors DF\n",
    "\n",
    "#         ##centre convolved ev (see fsl docs)\n",
    "#         diff = max(convolved_ev) - (max(convolved_ev) - min(convolved_ev))/2.0\n",
    "#         convolved_ev = convolved_ev - diff\n",
    "#         regressors['interaction'] = convolved_ev * ts #interaction regressor\n",
    "\n",
    "#         #orthogonalize noise regressors to speed up computation\n",
    "#         ts = vector_rejection(ts,regressors['ventricles'])\n",
    "#         ts = vector_rejection(ts,regressors['wm'])\n",
    "#         regressors[mask] = ts\n",
    "\n",
    "#         #write outpt\n",
    "#         regressors.to_csv(out_f, header=True,index = False, columns = ['wm','ventricles',mask,'interaction','run'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_design(in_tuple):\n",
    "    mask = 'hipp'\n",
=======
    "def write_design(in_tuple):\n",
    "    mask = 'vta'\n",
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
    "    sub,exp = in_tuple\n",
    "    #hrf params\n",
    "    hrf = getattr(glm,'GammaDifferenceHRF')\n",
    "    tr = 1.5\n",
    "    hrf = hrf(tr = tr)\n",
    "\n",
    "    out_f = home_dir + 'data/' + sub + '/design/ppi_regressors_' + exp + '_' + mask + '.csv' #out file\n",
    "\n",
    "    if True:#not os.path.exists(out_f):\n",
    "        #load design data for this subjects \n",
    "        design_dir = home_dir + 'data/' + sub + '/design/'\n",
<<<<<<< HEAD
    "        design_file = design_dir + exp + '_one_condition.csv'\n",
=======
    "        design_file = design_dir + exp + '_ppi.csv'\n",
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
    "        design_data = pd.read_csv(design_file)\n",
    "\n",
    "        #load in pre-existing noise regressors\n",
    "        reg_file = design_dir + 'noise_regressors_' + exp + '.csv'\n",
<<<<<<< HEAD
    "        regressors = pd.DataFrame()\n",
=======
    "        regressors = pd.read_csv(reg_file)\n",
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
    "\n",
    "        #initialize vars to fill\n",
    "        convolved_ev = []\n",
    "        ts = []\n",
<<<<<<< HEAD
    "        run_col = []\n",
    "        for run in runs:\n",
    "            sub_file = home_dir + 'analysis/' + exp + '_4mm/' + sub + '/preproc/run_' + str(run) + '/smoothed_timeseries.nii.gz'\n",
    "\n",
    "            if os.path.exists(sub_file):\n",
    "                ntp = nib.load(sub_file).shape[-1] #get number of time points\n",
    "                run_col.extend(np.repeat(run,ntp))\n",
    "                design = design_data[design_data['run']==int(run)]\n",
    "\n",
    "                model = glm.DesignMatrix(design = design, tr = tr, ntp = ntp, hrf_model = hrf, hpf_cutoff = 128)\n",
    "                convolved_ev.extend(model.design_matrix['event'].values) #get timeseries for regressor of interest\n",
    "\n",
    "                #load ts data\n",
    "                mask_dir = home_dir + 'data/' + sub + '/masks/extractions/' \n",
=======
    "        for run in runs:\n",
    "            sub_file = home_dir + 'analysis/' + exp + '_4mm/' + sub + '/preproc/run_' + str(run) + '/unsmoothed_timeseries.nii.gz'\n",
    "\n",
    "            if os.path.exists(sub_file):\n",
    "                ntp = nib.load(sub_file).shape[-1] #get number of time points\n",
    "                design = design_data[design_data['run']==int(run)]\n",
    "\n",
    "                model = glm.DesignMatrix(design = design, tr = tr, ntp = ntp, hrf_model = hrf, hpf_cutoff = 128)\n",
    "                convolved_ev.extend(model.design_matrix['state'].values) #get timeseries for regressor of interest\n",
    "\n",
    "                #load ts data\n",
    "                mask_dir = home_dir + 'data/' + sub + '/masks/vta/extractions/' \n",
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
    "                fid = (sub,exp,run,mask)\n",
    "                mask_f = mask_dir + ('').join(map(str,fid))+ '.txt'\n",
    "                roi_ts = np.loadtxt(mask_f)\n",
    "                roi_ts = roi_ts - np.mean(roi_ts) #mean center\n",
    "                ts.extend(roi_ts)\n",
    "\n",
    "        #update regressors dataframe\n",
    "        ts = scipy.stats.zscore(ts) #add ts to the regressors DF\n",
<<<<<<< HEAD
    "        regressors[mask] = ts\n",
    "        \n",
    "        #add in run idx\n",
    "        regressors['run'] = run_col\n",
=======
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
    "\n",
    "        ##centre convolved ev (see fsl docs)\n",
    "        diff = max(convolved_ev) - (max(convolved_ev) - min(convolved_ev))/2.0\n",
    "        convolved_ev = convolved_ev - diff\n",
    "        regressors['interaction'] = convolved_ev * ts #interaction regressor\n",
    "\n",
<<<<<<< HEAD
    "        #write outpt\n",
    "        regressors.to_csv(out_f, header=True,index = False, columns = [mask,'interaction','run'])\n"
=======
    "        #orthogonalize noise regressors to speed up computation\n",
    "        ts = vector_rejection(ts,regressors['ventricles'])\n",
    "        ts = vector_rejection(ts,regressors['wm'])\n",
    "        regressors[mask] = ts\n",
    "\n",
    "        #write outpt\n",
    "        regressors.to_csv(out_f, header=True,index = False, columns = ['wm','ventricles',mask,'interaction','run'])"
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 130,
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "exps = ['sim']\n",
    "in_tuples = []\n",
    "for sub in sub_list:\n",
    "    for exp in exps:\n",
    "        in_tuples.append((sub,exp))"
=======
    "exps = ['ser','sim']\n",
    "in_tuples = []\n",
    "for sub in sub_list:\n",
    "    if True:\n",
    "        for exp in exps:\n",
    "            in_tuples.append((sub,exp))"
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
=======
   "execution_count": 131,
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(processes = len(in_tuples))\n",
    "pool.map(write_design,in_tuples)\n",
    "pool.terminate()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
    "collapsed": true
=======
    "collapsed": false
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
<<<<<<< HEAD
   "version": "2.7.11"
=======
   "version": "2.7.6"
>>>>>>> 38d7e1606f641b3d1e7298d6eca48723532b6159
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
