{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#os and i/o\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from os.path import abspath\n",
    "import csv\n",
    "\n",
    "#scientific computing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats, optimize\n",
    "from pandas import DataFrame, Series\n",
    "import seaborn as sns\n",
    "import random as rd\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import scipy.stats\n",
    "\n",
    "#ipython add-ons\n",
    "from IPython.parallel import Client\n",
    "from IPython.display import Image\n",
    "import multiprocessing\n",
    "\n",
    "##nipype\n",
    "import nibabel as nib\n",
    "from nipype.pipeline.engine import Node, MapNode, Workflow\n",
    "from nipype.interfaces.io import DataGrabber, DataFinder, DataSink\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.interfaces.fsl import BET\n",
    "from nipype.interfaces.freesurfer.preprocess import ReconAll\n",
    "from nipype.interfaces.freesurfer.utils import MakeAverageSubject\n",
    "from nipype.interfaces.fsl import ExtractROI\n",
    "from nipype.interfaces.fsl import Merge\n",
    "from nipype.interfaces.fsl import TOPUP\n",
    "from nipype.interfaces.fsl import ApplyTOPUP\n",
    "from nipype.interfaces.fsl import ExtractROI\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preliminary housekeeping\n",
    "home_dir = '/data/home/iballard/fd/'\n",
    "subj_file = home_dir + 'scripts/sub_cb_mappings.txt'\n",
    "acq_params = home_dir + 'scripts/acqparams.txt'\n",
    "os.chdir(home_dir)\n",
    "\n",
    "num_slices = 57 #if you don't know, use nib.load and shape attribute\n",
    "#get subject list\n",
    "sub_list = []\n",
    "with open(subj_file) as f:\n",
    "    for line in f:\n",
    "        (key,val) = line.strip().split(' ') \n",
    "        sub_list.append('fd_' + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function which adds slices to scans for topup (which neeeds even number of slices)\n",
    "def add_slice(scan):\n",
    "    old_file = abspath(scan)\n",
    "    new_file = abspath(scan[:-7] + '_' + str(num_slices + 1)) + '.nii.gz'\n",
    "    if not os.path.exists(new_file): #only run if target doesnt exist. \n",
    "        fslroi = ExtractROI(in_file=old_file, roi_file=new_file,\n",
    "                            x_min=0,x_size=-1,y_min=0,y_size=-1,\n",
    "                            z_min=0,z_size= (num_slices + 1))\n",
    "        fslroi.run()\n",
    "    return os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##get list of scans to run\n",
    "scans = []\n",
    "for sub in sub_list:\n",
    "    design_dir = os.path.exists('data/' + sub + '/design') #create design files AFTER \n",
    "    \n",
    "    if not design_dir:\n",
    "        scans.extend(glob.glob('data/' + sub + '/cal/*')) #calibration scans\n",
    "        scans.extend(glob.glob('data/' + sub + '/func/*/*.nii.gz')) #functional scans   \n",
    "scans = [s for s in scans if len(s.split('/')[-1].split('_'))==2] #just scans that havent been ru\n",
    "scans = ['data/fd_141/func/ser/run_1.nii.gz','data/fd_143/func/ser/run_1.nii.gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run analysis\n",
    "pool = multiprocessing.Pool(processes = 8)\n",
    "pool.map(add_slice,scans)\n",
    "pool.terminate()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##take a timepoint of the gradient reversal scans\n",
    "def slice_scans(sub):\n",
    "    identifier = str(num_slices + 1) #file identifier\n",
    "    scans = glob.glob('data/' + sub + '/cal/*' + identifier + '.nii.gz') #padded calibration scans\n",
    "    for scan in scans:\n",
    "        old_file = abspath(scan)\n",
    "        new_file = abspath(scan[:-7] + '_slice.nii.gz')\n",
    "        if not os.path.exists(new_file):\n",
    "            fslroi = ExtractROI(in_file=old_file, roi_file=new_file,\n",
    "                                t_min=1,t_size=1)\n",
    "            fslroi.run()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = multiprocessing.Pool()\n",
    "pool.map(slice_scans,sub_list)\n",
    "pool.terminate()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##merge the calibration scans\n",
    "def merge_scans(sub):\n",
    "    identifier = 'slice' #file identifier\n",
    "    cal1 = glob.glob('data/' + sub + '/cal/cal1*' + identifier + '*') #padded calibration scans\n",
    "    cal2 = glob.glob('data/' + sub + '/cal/cal2*' + identifier + '*') #padded calibration scans\n",
    "\n",
    "    if sub == 'fd_115': #extra set of scans for this subject (see notes)\n",
    "        num_scans = 5\n",
    "    else:\n",
    "        num_scans = 4\n",
    "    for i in range(1,num_scans+1):\n",
    "        pe1 = [s for s in cal1 if int(s.split('/')[-1].split('_')[1]) == i][0]\n",
    "        pe0 = [s for s in cal2 if int(s.split('/')[-1].split('_')[1]) == i][0]\n",
    "        out = abspath('data/' + sub + '/cal/b0_both_' + str(i) + '.nii.gz')\n",
    "        if not os.path.exists(out):\n",
    "            merger = Merge(in_files=[pe0,pe1], merged_file=out,dimension = 't')\n",
    "            merger.run()    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = multiprocessing.Pool()\n",
    "pool.map(merge_scans,sub_list)\n",
    "pool.terminate()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#execute topup\n",
    "def topup_scans(scan):\n",
    "    new_file = abspath(scan[:-7] + '_topup') \n",
    "    unwarped = abspath(scan[:-7] + '_uw') \n",
    "    log = abspath(scan[:-7] + '_log') \n",
    "    field = abspath(scan[:-7] + '_field')\n",
    "    if not os.path.exists(field +'.nii.gz'): #only run if not already run\n",
    "        topup = TOPUP(in_file=scan, encoding_file = acq_params, out_base = new_file,\n",
    "                      out_corrected = unwarped, out_logfile = log,out_field = field)\n",
    "        topup.run()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get list of cal scans to apply topup to\n",
    "scans = []\n",
    "for sub in sub_list:\n",
    "    design_dir = os.path.exists('data/' + sub + '/design') #create design files AFTER \n",
    "    if not design_dir:\n",
    "        scans.extend(glob.glob('data/' + sub + '/cal/b0*.nii.gz')) #padded calibration scans\n",
    "scans = [s for s in scans if len(s.split('/')[-1].split('_'))==3] #just take raw images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run analysis\n",
    "pool = multiprocessing.Pool()\n",
    "pool.map(topup_scans,scans)\n",
    "pool.terminate()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##apply topup to functional runs. This has some experiment specific organizing\n",
    "##because I acquire enough data for 4 fieldmaps throughout the experiment\n",
    "def apply_topup(scan_tuple):\n",
    "    #padded functional scans\n",
    "    sub, scan  = scan_tuple\n",
    "    out = scan[:-10] + '_fc.nii.gz' #output file name\n",
    "    if not os.path.exists(out): #only run if it hasn't been run already\n",
    "        #get exp id (sim, loc, func) and run #\n",
    "        exp = scan.split('/')[3] \n",
    "        run = int(scan.split('/')[4].split('_')[1])\n",
    "\n",
    "        if exp == 'loc': #localizers were run last\n",
    "            cal_scan = 4\n",
    "        elif run == 1:\n",
    "            cal_scan = 1\n",
    "        elif run == 2:\n",
    "            cal_scan = 2\n",
    "        elif run == 3:\n",
    "            cal_scan = 3\n",
    "\n",
    "        #for this subject, run2 cal scans are junk\n",
    "        if sub == 'fd_101' and run == 2 :\n",
    "            cal_scan = 1\n",
    "\n",
    "        #one subject has extra cal scan for 2nd localizer run on a different day\n",
    "        if sub == 'fd_115' and exp == 'loc' and run == 2:\n",
    "            cal_scan = 5\n",
    "\n",
    "        if sub == 'fd_127' and exp == 'sim' and run == 2:\n",
    "            cal_scan = 1\n",
    "\n",
    "        #get correct field inputs\n",
    "        base_file = abspath('data/' + sub + '/cal/b0_both_' + str(cal_scan))\n",
    "        movpar = base_file + '_topup_movpar.txt'\n",
    "        fieldcoef = base_file + '_topup_fieldcoef.nii.gz'\n",
    "\n",
    "        applytopup = ApplyTOPUP(in_files = scan, encoding_file = acq_params,\n",
    "                                out_corrected = out, in_index = [1], method = 'jac',\n",
    "                                in_topup_movpar = movpar, in_topup_fieldcoef = fieldcoef)  \n",
    "        applytopup.run()            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fd_141', 'data/fd_141/func/ser/run_1_58.nii.gz'),\n",
       " ('fd_143', 'data/fd_143/func/ser/run_1_58.nii.gz')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get list of scans to apply topup to (in a tuple with subject identifier for convenience)\n",
    "scan_tuples = []\n",
    "for sub in sub_list:\n",
    "    design_dir = os.path.exists('data/' + sub + '/design') #create design files AFTER \n",
    "    if not design_dir:\n",
    "        scans = glob.glob('data/' + sub + '/func/*/*' + str(num_slices + 1) + '.nii.gz')\n",
    "        for s in scans:\n",
    "            scan_tuples.append((sub,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/nipype/interfaces/base.py:397: UserWarning: Input in_topup_movpar requires inputs: in_topup_fieldcoef\n",
      "  warn(msg)\n",
      "/usr/lib/python2.7/dist-packages/nipype/interfaces/base.py:397: UserWarning: Input in_topup_movpar requires inputs: in_topup_fieldcoef\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(processes = 8)\n",
    "pool.map(apply_topup,scan_tuples)\n",
    "pool.terminate()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fd_101\n",
      "fd_102\n",
      "fd_104\n",
      "fd_105\n",
      "fd_106\n",
      "fd_107\n",
      "fd_108\n",
      "fd_109\n",
      "fd_110\n",
      "fd_112\n",
      "fd_113\n",
      "fd_114\n",
      "fd_115\n",
      "fd_117\n",
      "fd_118\n",
      "fd_119\n",
      "fd_120\n",
      "fd_122\n",
      "fd_123\n",
      "fd_124\n",
      "fd_126\n",
      "fd_127\n",
      "fd_128\n",
      "fd_129\n",
      "fd_130\n",
      "fd_132\n",
      "fd_133\n"
     ]
    }
   ],
   "source": [
    "#Basic double checking based on files size. Mostly will check for processes prememptively\n",
    "#killed due to memory overload\n",
    "for sub in sub_list:\n",
    "    scans = glob.glob('data/' + sub + '/func/*/*fc.nii.gz')\n",
    "    b0 = glob.glob('data/' + sub + '/cal/*field.nii.gz')\n",
    "    if len(scans) != 8:\n",
    "        print sub\n",
    "    #check b0 for file size\n",
    "    for b in b0:\n",
    "        file_size = np.round(os.path.getsize(b)/1000.0)\n",
    "        if file_size < 4000: #less than 4 MB\n",
    "            print b\n",
    "    \n",
    "    #check field corrected functionals for file size\n",
    "    for scan in scans:\n",
    "        file_size = np.round(os.path.getsize(scan)/1000000.0)\n",
    "        orig_file = scan[:-10] + '.nii.gz' \n",
    "        orig_size = np.round(os.path.getsize(orig_file)/1000000.0)\n",
    "        if file_size < 400: #scan less than 400 MB\n",
    "            print scan\n",
    "        \n",
    "        if abs(file_size - orig_size) > 100: #big difference between file sizes\n",
    "            print abs(file_size - orig_size)\n",
    "            print scan            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Specific to this experiment. Sanity check that ser/sim assignment has been done correctly by\n",
    "##examining number of timepoints\n",
    "for sub in sub_list:\n",
    "    loc = glob.glob('data/' + sub + '/func/loc/*58*')\n",
    "    sim = glob.glob('data/' + sub + '/func/sim/*58*')\n",
    "    ser = glob.glob('data/' + sub + '/func/ser/*58*')\n",
    "    \n",
    "    for scan in loc:\n",
    "        img = nib.load(scan)\n",
    "        time = img.shape[3]*1.5\n",
    "        if time < 300 or time > 400: #should be around 5.5 minutes\n",
    "            print scan\n",
    "            print time\n",
    "    for scan in sim:\n",
    "        img = nib.load(scan)\n",
    "        time = img.shape[3]*1.5\n",
    "        if time < 360 or time > 480: #should be around 7 minutes\n",
    "            print scan\n",
    "            print time\n",
    "    for scan in ser:\n",
    "        img = nib.load(scan)\n",
    "        time = img.shape[3]*1.5\n",
    "        if time < 450 or time > 600: #should be around 8.5 minutes\n",
    "            print scan\n",
    "            print time\n",
    "            \n",
    "#Note to self: First subject had longer scans because I was worried about stopping the scanner early\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
