{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#os and i/o\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from os.path import abspath\n",
    "import csv\n",
    "\n",
    "#scientific computing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats, optimize\n",
    "from pandas import DataFrame, Series\n",
    "import seaborn as sns\n",
    "import random as rd\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import scipy.stats\n",
    "\n",
    "#ipython add-ons\n",
    "from IPython.parallel import Client\n",
    "from IPython.display import Image\n",
    "import multiprocessing\n",
    "\n",
    "##nipype\n",
    "import nibabel as nib\n",
    "from nipype.pipeline.engine import Node, MapNode, Workflow\n",
    "from nipype.interfaces.io import DataGrabber, DataFinder, DataSink\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.interfaces.fsl import BET\n",
    "from nipype.interfaces.freesurfer.preprocess import ReconAll\n",
    "from nipype.interfaces.freesurfer.utils import MakeAverageSubject\n",
    "from nipype.interfaces.fsl import ExtractROI\n",
    "from nipype.interfaces.fsl import Merge\n",
    "from nipype.interfaces.fsl import TOPUP\n",
    "from nipype.interfaces.fsl import ApplyTOPUP\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preliminary housekeeping\n",
    "home_dir = '/data/home/iballard/fd/'\n",
    "subj_file = home_dir + 'scripts/sub_cb_mappings.txt'\n",
    "acq_params = home_dir + 'scripts/acqparams.txt'\n",
    "os.chdir(home_dir)\n",
    "\n",
    "num_slices = 57 #if you don't know, use nib.load and shape attribute\n",
    "#get subject list\n",
    "sub_list = []\n",
    "with open(subj_file) as f:\n",
    "    for line in f:\n",
    "        (key,val) = line.strip().split(' ') \n",
    "        sub_list.append('fd_' + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_slice(sub):\n",
    "    import os\n",
    "    import glob\n",
    "    from nipype.interfaces.fsl import ExtractROI\n",
    "    scans = glob.glob('data/' + sub + '/cal/*') #calibration scans\n",
    "    scans.extend(glob.glob('data/' + sub + '/func/*/*.nii.gz')) #functional scans\n",
    "    scans = [s for s in scans if len(s.split('/')[-1].split('_'))==2] #just scans that havent been run\n",
    "    \n",
    "    for scan in scans:\n",
    "        old_file = abspath(scan)\n",
    "        new_file = abspath(scan[:-7] + '_' + str(num_slices + 1)) + '.nii.gz'\n",
    "        if not os.path.exists(new_file): #only run if target doesnt exist. \n",
    "            fslroi = ExtractROI(in_file=old_file, roi_file=new_file,\n",
    "                                x_min=0,x_size=-1,y_min=0,y_size=-1,\n",
    "                                z_min=0,z_size= (num_slices + 1))\n",
    "            fslroi.run()\n",
    "    return os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9126,\n",
       " 9128,\n",
       " 9133,\n",
       " 9132,\n",
       " 9130,\n",
       " 9126,\n",
       " 9132,\n",
       " 9126,\n",
       " 9130,\n",
       " 9128,\n",
       " 9126,\n",
       " 9133,\n",
       " 9126,\n",
       " 9126,\n",
       " 9132]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##add a z slice to all scans\n",
    "\n",
    "#delete old versions. Uncomment if re-running and debugging\n",
    "# for sub in sub_list:\n",
    "#     scans = glob.glob('data/' + sub + '/cal/*58*') #calibration scans\n",
    "#     scans.extend(glob.glob('data/' + sub + '/func/*/*58*.nii.gz')) #functional scans    \n",
    "#     for scan in scans:\n",
    "#         os.remove(scan)\n",
    "\n",
    "pool = multiprocessing.Pool(processes = 5)\n",
    "pool.map(add_slice,sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##take a timepoint of the gradient reversal scans\n",
    "def slice_scans(sub):\n",
    "    identifier = str(num_slices + 1) #file identifier\n",
    "    scans = glob.glob('data/' + sub + '/cal/*' + identifier + '*') #padded calibration scans\n",
    "    for scan in scans:\n",
    "        old_file = abspath(scan)\n",
    "        new_file = abspath(scan[:-7] + '_slice.nii.gz')\n",
    "        fslroi = ExtractROI(in_file=old_file, roi_file=new_file,\n",
    "                            t_min=1,t_size=1)\n",
    "        fslroi.run()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exectute. Delete old files first if debugging\n",
    "# for sub in sub_list:\n",
    "#     scans = glob.glob('data/' + sub + '/cal/*slice*') #calibration scans\n",
    "#     for scan in scans:\n",
    "#         os.remove(scan)\n",
    "pool = multiprocessing.Pool()\n",
    "pool.map(slice_scans,sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##merge the calibration scans\n",
    "def merge_scans(sub):\n",
    "    identifier = 'slice' #file identifier\n",
    "    cal1 = glob.glob('data/' + sub + '/cal/cal1*' + identifier + '*') #padded calibration scans\n",
    "    cal2 = glob.glob('data/' + sub + '/cal/cal2*' + identifier + '*') #padded calibration scans\n",
    "\n",
    "    if sub == 'fd_115': #extra set of scans for this subject (see notes)\n",
    "        num_scans = 5\n",
    "    else:\n",
    "        num_scans = 4\n",
    "    \n",
    "    for i in range(1,num_scans+1):\n",
    "        pe1 = [s for s in cal1 if int(s.split('/')[-1].split('_')[1]) == i][0]\n",
    "        pe0 = [s for s in cal2 if int(s.split('/')[-1].split('_')[1]) == i][0]\n",
    "        out = abspath('data/' + sub + '/cal/b0_both_' + str(i) + '.nii.gz')\n",
    "        merger = Merge(in_files=[pe0,pe1], merged_file=out,dimension = 't')\n",
    "        merger.run()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:interface:stderr 2015-04-29T21:59:52.340143:\n",
      "INFO:interface:stderr 2015-04-29T21:59:52.340143:WARNING:: Inconsistent orientations for individual images when attempting to merge.\n",
      "INFO:interface:stderr 2015-04-29T21:59:52.340143:          Merge will use voxel-based orientation which is probably incorrect - *PLEASE CHECK*!\n",
      "INFO:interface:stderr 2015-04-29T21:59:52.340143:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = multiprocessing.Pool()\n",
    "pool.map(merge_scans,sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#execute topup\n",
    "def topup_scans(sub):\n",
    "    scans = glob.glob('data/' + sub + '/cal/b0*') #padded calibration scans\n",
    "    for scan in scans:\n",
    "        new_file = abspath(scan[:-7] + '_topup') \n",
    "        unwarped = abspath(scan[:-7] + '_uw') \n",
    "        log = abspath(scan[:-7] + '_log') \n",
    "        field = abspath(scan[:-7] + '_field') \n",
    "        topup = TOPUP(in_file=scan, encoding_file = acq_params, out_base = new_file,\n",
    "                     out_corrected = unwarped, out_logfile = log,out_field = field)\n",
    "        topup.run()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool()\n",
    "pool.map(topup_scans,sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##apply topup to functional runs. This has some experiment specific organizing\n",
    "##because I acquire enough data for 4 fieldmaps throughout the experiment\n",
    "def apply_topup(sub):\n",
    "    #padded functional scans\n",
    "    scans = glob.glob('data/' + sub + '/func/*/*' + str(num_slices + 1) + '.nii.gz') \n",
    "    \n",
    "    for scan in scans:\n",
    "        out = scan[:-10] + '_fc.nii.gz' #output file name\n",
    "        if not os.path.exists(out): #only run if it hasn't been run already\n",
    "            #get exp id (sim, loc, func) and run #\n",
    "            exp = scan.split('/')[3] \n",
    "            run = int(scan.split('/')[4].split('_')[1])\n",
    "\n",
    "            if exp == 'loc': #localizers were run last\n",
    "                cal_scan = 4\n",
    "            elif run == 1:\n",
    "                cal_scan = 1\n",
    "            elif run == 2:\n",
    "                cal_scan = 2\n",
    "            elif run == 3:\n",
    "                cal_scan = 3\n",
    "\n",
    "            #one subject has extra cal scan for 2nd localizer run on a different day\n",
    "            if sub == 'fd_115' and exp == 'loc' and run == 2:\n",
    "                cal_scan = 5\n",
    "\n",
    "            #get correct field inputs\n",
    "            base_file = abspath('data/' + sub + '/cal/b0_both_' + str(cal_scan))\n",
    "            movpar = base_file + '_topup_movpar.txt'\n",
    "            fieldcoef = base_file + '_topup_fieldcoef.nii.gz'\n",
    "\n",
    "            applytopup = ApplyTOPUP(in_files = scan, encoding_file = acq_params,\n",
    "                                    out_corrected = out, in_index = [1], method = 'jac',\n",
    "                                    in_topup_movpar = movpar, in_topup_fieldcoef = fieldcoef)  \n",
    "            applytopup.run()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(processes = 8)\n",
    "pool.map(apply_topup,sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Basic double checking based on files size. Mostly will check for processes prememptively\n",
    "#killed due to memory overload\n",
    "for sub in sub_list:\n",
    "    scans = glob.glob('data/' + sub + '/func/*/*fc*')\n",
    "    b0 = glob.glob('data/' + sub + '/cal/*field.nii.gz')\n",
    "    if len(scans) != 8:\n",
    "        print sub\n",
    "    \n",
    "    #check b0 for file size\n",
    "    for b in b0:\n",
    "        file_size = np.round(os.path.getsize(b)/1000.0)\n",
    "        if file_size < 4000: #less than 4 MB\n",
    "            print b\n",
    "    \n",
    "    #check field corrected functionals for file size\n",
    "    for scan in scans:\n",
    "        file_size = np.round(os.path.getsize(scan)/1000000.0)\n",
    "        orig_file = scan[:-10] + '.nii.gz' \n",
    "        orig_size = np.round(os.path.getsize(orig_file)/1000000.0)\n",
    "        if file_size < 400: #scan less than 400 MB\n",
    "            print scan\n",
    "        \n",
    "        if abs(file_size - orig_size) > 100: #big difference between file sizes\n",
    "            print abs(file_size - orig_size)\n",
    "            print scan            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/fd_101/func/sim/run_2_58.nii.gz\n",
      "490.5\n",
      "data/fd_101/func/sim/run_1_58.nii.gz\n",
      "603.0\n",
      "data/fd_101/func/ser/run_2_58.nii.gz\n",
      "603.0\n"
     ]
    }
   ],
   "source": [
    "##Specific to this experiment. Sanity check that ser/sim assignment has been done correctly by\n",
    "##examining number of timepoints\n",
    "for sub in sub_list:\n",
    "    loc = glob.glob('data/' + sub + '/func/loc/*58*')\n",
    "    sim = glob.glob('data/' + sub + '/func/sim/*58*')\n",
    "    ser = glob.glob('data/' + sub + '/func/ser/*58*')\n",
    "    \n",
    "    for scan in loc:\n",
    "        img = nib.load(scan)\n",
    "        time = img.shape[3]*1.5\n",
    "        if time < 300 or time > 400: #should be around 5.5 minutes\n",
    "            print scan\n",
    "            print time\n",
    "    for scan in sim:\n",
    "        img = nib.load(scan)\n",
    "        time = img.shape[3]*1.5\n",
    "        if time < 360 or time > 480: #should be around 7 minutes\n",
    "            print scan\n",
    "            print time\n",
    "    for scan in ser:\n",
    "        img = nib.load(scan)\n",
    "        time = img.shape[3]*1.5\n",
    "        if time < 450 or time > 600: #should be around 8.5 minutes\n",
    "            print scan\n",
    "            print time\n",
    "            \n",
    "#Note to self: First subject had longer scans because I was worried about stopping the scanner early\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
