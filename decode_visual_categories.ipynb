{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#os and i/o\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from os.path import abspath\n",
    "import csv\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "#scientific computing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats, optimize, ndimage\n",
    "from pandas import DataFrame, Series\n",
    "import seaborn as sns\n",
    "import random as rd\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import scipy.stats\n",
    "import math\n",
    "\n",
    "#sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.cross_validation import LeaveOneLabelOut, cross_val_score, permutation_test_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import decomposition\n",
    "import pickle\n",
    "\n",
    "#ipython add-ons\n",
    "from IPython.parallel import Client\n",
    "from IPython.display import Image\n",
    "import multiprocessing\n",
    "\n",
    "##nipype\n",
    "import nibabel as nib\n",
    "from nipype.pipeline.engine import Node, MapNode, Workflow\n",
    "from nipype.interfaces.io import DataGrabber, DataFinder, DataSink\n",
    "from nipype.interfaces import fsl\n",
    "from nipype.interfaces.fsl import BET\n",
    "from nipype.interfaces.freesurfer.preprocess import ReconAll\n",
    "from nipype.interfaces.freesurfer.utils import MakeAverageSubject\n",
    "from nipype.interfaces.fsl import ExtractROI\n",
    "from nipype.interfaces.fsl import Merge\n",
    "from nipype.interfaces.fsl import TOPUP\n",
    "from nipype.interfaces.fsl import ApplyTOPUP\n",
    "from nipype.workflows.fmri.fsl import create_susan_smooth\n",
    "\n",
    "from nilearn import input_data\n",
    "import nilearn.decoding\n",
    "import lyman\n",
    "\n",
    "from surfer import Brain\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preliminary housekeeping\n",
    "home_dir = '/data/home/iballard/fd/'\n",
    "subj_file = home_dir + 'subjects.txt'\n",
    "sub_list = list(np.loadtxt(subj_file,'string'))\n",
    "os.chdir(home_dir)\n",
    "exp_list = ['loc']\n",
    "tr = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fd_148 loc 2 202 199\n"
     ]
    }
   ],
   "source": [
    "def load_data_roi_mean(sub,exp,roi,timing):\n",
    "\n",
    "    #roi mask\n",
    "    roi_mask = home_dir + 'data/' + sub + '/masks/VTC_occ_IT.nii.gz'\n",
    "    \n",
    "    #load mask func\n",
    "    mask_f = home_dir + 'analysis/' + exp + '/' + sub + '/reg/epi/unsmoothed/run_1/functional_mask_xfm.nii.gz'\n",
    "    mask_img = nib.load(mask_f)\n",
    "    \n",
    "    #load mean func (for use in getting image info)\n",
    "    mean_f = home_dir + 'analysis/' + exp + '/' + sub + '/reg/epi/unsmoothed/run_1/mean_func_xfm.nii.gz'\n",
    "    mean_img = nib.load(mean_f)    \n",
    "    \n",
    "    X_data = []\n",
    "    y = []\n",
    "    runs = []\n",
    "    for run in range(1,4):\n",
    "        func_f = home_dir + 'analysis/' + exp + '/' + sub + '/reg/epi/unsmoothed/run_' + \\\n",
    "            str(run) + '/timeseries_xfm.nii.gz'\n",
    "        \n",
    "        if os.path.exists(func_f):    \n",
    "            ts_img = nib.load(func_f)\n",
    "            nifti_masker = input_data.NiftiMasker(mask_img = roi_mask, standardize = True)\n",
    "            ts_data = nifti_masker.fit_transform(func_f)\n",
    "            \n",
    "            #get indices of events of interest and mask data\n",
    "            indices = np.array(timing[timing['run']==run]['tr_index'].values,dtype=int)\n",
    "\n",
    "            #deal with situation when scan is too short\n",
    "            scan_dur = ts_data.shape[0]\n",
    "            scan_size = 6\n",
    "            if scan_dur <= (indices[-1] + scan_size):\n",
    "                print sub,exp,str(run),str(scan_dur),str(indices[-1])\n",
    "                \n",
    "                #crop indices vector and take scans\n",
    "                cropped_indices = [x for x in indices if (x + scan_size)<scan_dur]                \n",
    "\n",
    "                #Drop unused entries from the timing dataframe\n",
    "                while len(cropped_indices) < len(indices):\n",
    "                    cropped_indices.append(np.NaN)\n",
    "                timing.loc[timing['run']==run,'tr_index'] = cropped_indices\n",
    "                timing = timing.dropna()\n",
    "                \n",
    "                indices = np.array([x for x in cropped_indices if not np.isnan(x)])\n",
    "                print sub,exp,str(run),str(scan_dur),str(indices[-1])\n",
    "\n",
    "            #take 4 scans relative to the stimulus onset\n",
    "            frame0 = ts_data[indices + 0,:]\n",
    "            frame1 = ts_data[indices + 2,:] \n",
    "            frame2 = ts_data[indices + 3,:]\n",
    "            frame3 = ts_data[indices + 4,:]\n",
    "            X_run = np.mean([frame1,frame2,frame3],axis=0) #average timepoints\n",
    "            X_run -= frame0\n",
    "            X_run = scipy.stats.zscore(X_run, axis = -1) #zscore\n",
    "            \n",
    "            y_run = timing[timing['run']==run]['condition'].values\n",
    "            run = timing[timing['run']==run]['run'].values\n",
    "            y_run = y_run[0:X_run.shape[0]]\n",
    "            run = run[0:X_run.shape[0]]\n",
    "\n",
    "            y.extend(y_run)\n",
    "            runs.extend(run)\n",
    "            X_data.append(X_run) #concatenate runs\n",
    "            \n",
    "    X = np.concatenate(X_data,axis=0)\n",
    "    runs = np.array(runs)\n",
    "    return mask_img, X, y, runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data_roi(sub,exp,roi,timing):\n",
    "\n",
    "    #roi mask\n",
    "    roi_mask = home_dir + 'data/' + sub + '/masks/VTC_occ_IT.nii.gz'\n",
    "    \n",
    "    #load mask func\n",
    "    mask_f = home_dir + 'analysis/' + exp + '/' + sub + '/reg/epi/unsmoothed/run_1/functional_mask_xfm.nii.gz'\n",
    "    mask_img = nib.load(mask_f)\n",
    "    \n",
    "    #load mean func (for use in getting image info)\n",
    "    mean_f = home_dir + 'analysis/' + exp + '/' + sub + '/reg/epi/unsmoothed/run_1/mean_func_xfm.nii.gz'\n",
    "    mean_img = nib.load(mean_f)    \n",
    "    \n",
    "    X_data = []\n",
    "    y = []\n",
    "    runs = []\n",
    "    for run in range(1,3):\n",
    "        func_f = home_dir + 'analysis/' + exp + '/' + sub + '/reg/epi/unsmoothed/run_' + \\\n",
    "            str(run) + '/timeseries_xfm.nii.gz'\n",
    "        \n",
    "        if os.path.exists(func_f):    \n",
    "            ts_img = nib.load(func_f)\n",
    "            nifti_masker = input_data.NiftiMasker(mask_img = roi_mask, standardize = True)\n",
    "            ts_data = nifti_masker.fit_transform(func_f)\n",
    "            \n",
    "            #get indices of events of interest and mask data\n",
    "            indices = np.array(timing[timing['run']==run]['tr_index'].values,dtype=int)\n",
    "\n",
    "            #deal with situation when scan is too short\n",
    "            scan_dur = ts_data.shape[0]\n",
    "            n_scans = 4\n",
    "            if scan_dur <= (indices[-1] + n_scans):\n",
    "                print sub,exp,str(run),str(scan_dur),str(indices[-1])\n",
    "                \n",
    "                #crop indices vector and take scans\n",
    "                cropped_indices = [x for x in indices if (x + n_scans)<scan_dur]                \n",
    "\n",
    "                #Drop unused entries from the timing dataframe\n",
    "                while len(cropped_indices) < len(indices):\n",
    "                    cropped_indices.append(np.NaN)\n",
    "                timing.loc[timing['run']==run,'tr_index'] = cropped_indices\n",
    "                timing = timing.dropna()\n",
    "                \n",
    "                indices = np.array([x for x in cropped_indices if not np.isnan(x)])\n",
    "                \n",
    "            y_run = timing[timing['run']==run]['condition'].values\n",
    "            run = timing[timing['run']==run]['run'].values\n",
    "            \n",
    "            #take all scans up to next stimulus onset, offset by 3s (2 scans)\n",
    "            take_index = []\n",
    "            y_scan = []\n",
    "            run_scan = []\n",
    "            baseline_index = []\n",
    "            for n,i in enumerate(indices):\n",
    "                if n < len(indices) - 1:\n",
    "                    diff = indices[n+1] - i\n",
    "                else:\n",
    "                    diff = scan_dur - i - 2\n",
    "            \n",
    "                for j in range(3,2+int(diff)):\n",
    "                    baseline_index.append(i)\n",
    "                    take_index.append(i + j)\n",
    "                    y_scan.append(y_run[n])\n",
    "                    run_scan.append(run[n])\n",
    "            \n",
    "#             baseline = (np.array(ts_data[baseline_index,:]) +\\\n",
    "#                         np.array(ts_data[np.array(baseline_index) - 1,:]))/2.0\n",
    "            X_data.append(ts_data[take_index,:] -  ts_data[baseline_index,:])\n",
    "            y.extend(y_scan)\n",
    "            runs.extend(run_scan)\n",
    "            \n",
    "    X = np.concatenate(X_data,axis=0)\n",
    "    runs = np.array(runs)\n",
    "    return mask_img, X, y, runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##add a column to the timing file coding for the TR in which an event occured\n",
    "def get_event_times(sub,exp):\n",
    "    event_file = home_dir + 'data/' + sub + '/design/' + exp + '_squashed.csv'\n",
    "    timing = pd.read_csv(event_file)\n",
    "    timing =  timing.sort(['run','onset'])\n",
    "    for run in range(1,3):\n",
    "        event_time = timing.loc[timing['run']==run,'onset'].values\n",
    "        tr_index = map(lambda x: int(math.floor(x/tr)),event_time)\n",
    "        timing.loc[timing['run']==run,'tr_index'] = tr_index\n",
    "    \n",
    "    return timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(in_tuple):\n",
    "    sub,exp,roi = in_tuple\n",
    "\n",
    "    timing = get_event_times(sub,exp)\n",
    "    mask_img, X, y, runs = load_data_roi(sub,exp,roi,timing)\n",
    "    y = np.array(map(lambda x: cond_map[x], y))\n",
    "    \n",
    "    return mask_img, X, y, runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_roi_mvpa(in_tuple):\n",
    "    sub,exp,roi = in_tuple\n",
    "\n",
    "    timing = get_event_times(sub,exp)\n",
    "    mask_img, X, y, runs = load_data_roi(sub,exp,roi,timing)\n",
    "    y = np.array(map(lambda x: cond_map[x], y))\n",
    "\n",
    "    y_new = np.array([x for x in y if x in [0,1,2]])\n",
    "    X_new = np.array([x for n,x in enumerate(X) if y[n] in [0,1,2]])\n",
    "    run_new = np.array([x for n,x in enumerate(runs) if y[n] in [0,1,2]])\n",
    "\n",
    "#     pca = decomposition.PCA(n_components = 100, whiten = True)\n",
    "\n",
    "#     classifier = OneVsRestClassifier(SVC(kernel = 'linear', C=.1, class_weight='auto'), n_jobs=-1)\n",
    "    classifier = LogisticRegression(C=1)\n",
    "#     classifier = RidgeClassifier(alpha = 1, class_weight='auto', fit_intercept=False, normalize=True)\n",
    "    selector = SelectPercentile(f_classif, percentile = 10)\n",
    "    clf = Pipeline([('anova',selector),('classification',classifier)])\n",
    "#     clf = Pipeline([('pca',pca),('classification',classifier)])\n",
    "\n",
    "    cv = LeaveOneLabelOut(run_new)\n",
    "\n",
    "#     rfe = RFE(estimator = classifier, step = 10, n_features_to_select=int(X.shape[1]/4))\n",
    "\n",
    "    res = cross_val_score(clf, X_new, y_new, cv=10, scoring = 'accuracy')\n",
    "\n",
    "\n",
    "    return (sub,exp,roi,np.mean(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_roi_mvpa_all_localizer(in_tuple):\n",
    "    sub,exp,roi = in_tuple\n",
    "\n",
    "    timing = get_event_times(sub,exp)\n",
    "    mask_img, X, y, runs = load_data_roi(sub,exp,roi,timing)\n",
    "    y = np.array(map(lambda x: cond_map[x], y))\n",
    "    out_f = home_dir + '/mvpa/' + sub\n",
    "    \n",
    "    y_new = np.array([x for x in y if x in [0,1,2]])\n",
    "    X_new = np.array([x for n,x in enumerate(X) if y[n] in [0,1,2]])\n",
    "\n",
    "    clf = LogisticRegression(C=1)\n",
    "    clf.fit(X_new,y_new)\n",
    "    s = pickle.dump(clf)\n",
    "\n",
    "    return (sub,exp,roi,np.mean(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fd_110 loc 1 202 199\n",
      "fd_110 loc 2 202 199\n"
     ]
    }
   ],
   "source": [
    "mask_img, X, y, runs = extract_data(in_tuples[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# y_new = np.array([x for x in y if x in [0,1,2]])\n",
    "# X_new = np.array([x for n,x in enumerate(X) if y[n] in [0,1,2]])\n",
    "# run_new = np.array([x for n,x in enumerate(runs) if y[n] in [0,1,2]])\n",
    "\n",
    "\n",
    "# pca = decomposition.PCA(n_components = 25, whiten = True)\n",
    "# # classifier = GaussianNB()\n",
    "# # classifier = SVC(kernel = 'linear', C=.1, class_weight='auto')\n",
    "# #     classifier = OneVsRestClassifier(LogisticRegression(C=1), n_jobs=-1)\n",
    "# classifier = RidgeClassifier(alpha = 1, class_weight='auto', fit_intercept=False, normalize=True)\n",
    "# # selector = SelectPercentile(f_classif, percentile = 10)\n",
    "# # clf = Pipeline([('anova',selector),('classification',classifier)])\n",
    "# clf = Pipeline([('pca',pca),('classification',classifier)])\n",
    "\n",
    "# # cv = LeaveOneLabelOut(run_new)\n",
    "# kf = StratifiedKFold(y_new,n_folds = 10)\n",
    "\n",
    "# for train_index, test_index in kf:\n",
    "#     X_train, X_test = X_new[train_index], X_new[test_index]\n",
    "#     y_train, y_test = y_new[train_index], y_new[test_index]\n",
    "# a = clf.fit(X_train,y_train)\n",
    "\n",
    "# # res = cross_val_score(clf, X_new, y_new, cv=cv, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "in_tuples = []\n",
    "cond_map = {'face':0,'body':1, 'place': 2, 'character':3, 'object':4}  \n",
    "roi = 'VTC'\n",
    "for exp in ['loc']:\n",
    "    for sub in sub_list:\n",
    "        in_tuples.append((sub,exp,roi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pool = multiprocessing.Pool(processes = 18)\n",
    "# output = pool.map(run_roi_mvpa,in_tuples)\n",
    "# pool.terminate()\n",
    "# pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.672686887255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.060782432111484749"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs = []\n",
    "for o in output:\n",
    "    s,e,r,acc = o\n",
    "    accs.append(acc)\n",
    "print np.mean(accs)\n",
    "np.std(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fd_104', 'loc', 'VTC', 0.66985294117647054),\n",
       " ('fd_105', 'loc', 'VTC', 0.42826797385620913),\n",
       " ('fd_107', 'loc', 'VTC', 0.54223856209150334),\n",
       " ('fd_108', 'loc', 'VTC', 0.74158496732026147),\n",
       " ('fd_109', 'loc', 'VTC', 0.62647058823529422),\n",
       " ('fd_110', 'loc', 'VTC', 0.6872140522875817),\n",
       " ('fd_112', 'loc', 'VTC', 0.59215686274509804),\n",
       " ('fd_113', 'loc', 'VTC', 0.58696895424836604),\n",
       " ('fd_114', 'loc', 'VTC', 0.65980392156862744),\n",
       " ('fd_115', 'loc', 'VTC', 0.64424019607843142),\n",
       " ('fd_117', 'loc', 'VTC', 0.63590686274509811),\n",
       " ('fd_118', 'loc', 'VTC', 0.62160947712418291),\n",
       " ('fd_119', 'loc', 'VTC', 0.69207516339869279),\n",
       " ('fd_122', 'loc', 'VTC', 0.6556781045751634),\n",
       " ('fd_123', 'loc', 'VTC', 0.53656045751633985),\n",
       " ('fd_124', 'loc', 'VTC', 0.73480392156862739),\n",
       " ('fd_126', 'loc', 'VTC', 0.65951797385620914),\n",
       " ('fd_127', 'loc', 'VTC', 0.67160947712418295),\n",
       " ('fd_128', 'loc', 'VTC', 0.70910947712418293),\n",
       " ('fd_129', 'loc', 'VTC', 0.72749183006535945),\n",
       " ('fd_130', 'loc', 'VTC', 0.69660947712418297),\n",
       " ('fd_132', 'loc', 'VTC', 0.68239379084967322),\n",
       " ('fd_133', 'loc', 'VTC', 0.62928921568627449),\n",
       " ('fd_135', 'loc', 'VTC', 0.64624183006535962),\n",
       " ('fd_136', 'loc', 'VTC', 0.62581699346405228),\n",
       " ('fd_137', 'loc', 'VTC', 0.68378267973856199),\n",
       " ('fd_138', 'loc', 'VTC', 0.73966503267973849),\n",
       " ('fd_140', 'loc', 'VTC', 0.69734477124183003),\n",
       " ('fd_141', 'loc', 'VTC', 0.66781045751633994),\n",
       " ('fd_144', 'loc', 'VTC', 0.71466503267973858),\n",
       " ('fd_147', 'loc', 'VTC', 0.7084150326797386),\n",
       " ('fd_148', 'loc', 'VTC', 0.67720588235294121)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Test classifier performance on serial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_event_times_stim(sub,exp):\n",
    "    event_file = home_dir + 'data/' + sub + '/design/' + exp + '_PE.csv'\n",
    "    timing = pd.read_csv(event_file)\n",
    "    timing = timing[timing['condition'] != 'b_minus_PE']\n",
    "    timing = timing[timing['condition'] != 'c_minus_PE']\n",
    "    timing = timing[timing['condition'] != 'c_plus_PE']\n",
    "    timing = timing[timing['condition'] != 'b_plus_PE']\n",
    "    timing = timing[timing['condition'] != 'feedback_neg']\n",
    "    timing = timing[timing['condition'] != 'feedback_pos']\n",
    "\n",
    "    timing['stim'] = None\n",
    "\n",
    "    stim_mapping= {1:{'A':'face','b_minus':'body','b_plus':'body','c_minus':'house','c_plus':'house'},\n",
    "        2:{'A':'body','b_minus':'house','b_plus':'house','c_minus':'face','c_plus':'face'},\n",
    "         3:{'A':'house','b_minus':'face','b_plus':'face','c_minus':'body','c_plus':'body'}}\n",
    "\n",
    "    for run in set(timing['run'].values):\n",
    "        cond = timing[timing['run'] ==  run]['condition'].values\n",
    "        stim = map(lambda x: stim_mapping[run][x], cond)\n",
    "        timing.ix[timing['run'] ==  run,'stim'] = stim\n",
    "\n",
    "    timing['condition'] = timing['stim']\n",
    "    return timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
